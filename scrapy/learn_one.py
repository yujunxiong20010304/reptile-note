'''
-------------------------------------------------
记住：！！！创建一个工程后去吧工程中settings.py文件中的 ｜
ROBOTSTXT_OBEY = False #要改的                    ｜
LOG_LEVEL='ERROR' #要加的                         ｜
USER_AGENT 在这个后面加UA伪装就是headers那个         ｜
在settings指明图片存储的路径                        ｜
——————————————————————————————————————————————————
scrapy框架
    1。什么是scrapy框架
        集成了很多功能并且具有很强的通用性的项目模版
    2。如何学习scrapy框架
        专门学习框架封装的各种功能的详细用法
    3。什么是scrapy框架
        爬虫中封装好的明星框架
        功能：
            高性能的持久化存储
            异步数据下载
            高性能的数据解析操作
            分布式

scrapy基本使用
    创建一个工程：（先用 cd 跳转到你想创建的地方）
    ————————————————————————————————————
   ｜    scrapy startproject 工程名      ｜
    ————————————————————————————————————
        创建工程成功后会有一个文件夹出现，文件夹里面有一个spiders文件夹（是跟爬虫有关的），
        他的里面必须存放一个爬虫源文件
        settings.py是工程当前对应的一个配置文件
    在spiders子目录中创建一个爬虫文件(必须先 cd 到spiders文件这个工程目录中，再去创建)
    ————————————————————————————————————————————————
   ｜     scrapy genspider spiderName www.xxx.com   ｜
    ————————————————————————————————————————————————
        spiderName是爬虫名称  www.xxx.com 是起始的url
    执行工程：
    --------------------------------
    |    scrapy crawl spiderName   |
    --------------------------------
        spiderName是爬虫名称
        scrapy crawl spiderName --nolog 执行后不会打印日志，只打印执行结果，但是不会打印错误信息

    重点要写的代码要去爬虫文件中去写

scrapy如何实现数据解析：
    在scrapy当中是不需要获取响应数据的
    并且用xpath也不需要导包，不需要实现一个etree对象，而是直接调用xpath方法就行
    在scrapy的xpath中，数据解析返回的是列表，但列表元素一定是Selector类型的对象
    extract()可以将Selector对象中data参数存储的字符串提取出来
    !!!!我的理解：
        不管是谁调用了extract之后，则表示将列表中每一个Selector对象中data对应的字符串提取了出来
        xpath返回结果是一个列表，而这个列表中存储的是许多的Selector对象，所以对他使用后可以将
        data中的字符串提取出来

        ⚠️ 这个xpath和etree里面的xpath都可以用 ｜ 来进行或者

        extract_first()的作用：xpath的结果直接返回字符串，但你必须保证列表中只有一个元素

        开篇明义：get() 、getall() 是新版本的方法，extract() 、extract_first()是旧版本的方法。
        前者更好用，取不到就返回None，后者取不到就raise一个错误。


基于终端指令的持久化存储（两种方式）
    1。基于终端指令
        要求：只可以将parse方法的返回只存储到本地的文本文件中
        -------------------------------——————————————————+——
       ｜ scrapy crawl 程序文件名 -o 路径   #路径的最后是文件名 ｜
       ——————————————————————————————————————————————————————
       基于终端的文件存储，只能存储在json，jsonlines，jl，csv，xml，marshal，pickle这几种格式中
       好处：简介高效便捷
       局限：指定了文件格式
    2。基于管道
        编码流程：
            1。数据解析
            2。在item类中定义相关属性（在你创的工程里面）items.py
            3。解析的数据封装存储到item类型对象中
            4。将item类型的对象提交给管道进行持久化数据存储（在你创的工程里面） pipelines.py
            5。在管道类的process_item中要将其接收到的item对象中存储的数据进行持久化存储操作
            6。在配置文件中开启管道 配置文件是settings.py
        好处：
            通用性强


    面试提：
        将爬取到的数据一份存储到本地一份存储到数据库，如何实现
        管道文件中的一个管道类对应的是将数据库存储到一种平台
        爬虫文件提交的item只会给管道文件中第一个被执行的文件接受，
        process_item中的return item表示将item传递给下一个即将被执行的管道类




'''


