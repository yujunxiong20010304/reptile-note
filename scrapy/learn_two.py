'''
基于Spider的全站数据爬取
    就将这个网站中某板块下的全部页码对应的页面数据进行爬取
    需求：爬取校花网中的照片名称
    实现方式：
        将所有页面的url添加到start_urls列表中（不推荐）
        自行手动进行请求发送
        yeild scrapy.Request(url,callback) callback专门用作与数据解析，回调



        获取文本数据：
            text
        二进制数据：
            body



五大核心组件
    管道
    spider：产生url并请求发送，进行数据解析
    调度器
    下载器
    引擎

请求传参
    使用场景：
        如果要爬取解析的数据不再同一张页面中（深度爬取）
        需求：爬取boss的岗位名称，岗位描述


图片数据爬取ImagesPipeline
    基于scrapy爬取字符串类型的数据和爬取图片数据类型的区别
        字符串：只需要基于xpath解析且提交管道进行持久化存储
        图片：只可以通过xpath解析图片地址，还需要单独对图片地址发起请求获取图片二进制类型的数据
                scrapy单独封装了一个管道类（ImagesPipeline）爬取图片
        ImagesPipeLine:
            只需要将img的src的属性值进行解析，提交到管道，管道就会对图片的src进行请求发送获取图片的二进制类型的数据，且还会帮我们持久化存储。
        需求：爬取站长素材的高清图片
        使用流程：
                数据解析（图片地址）
                将存储图片地址的item提交到制定的管道类
                在管道文件中自定制一个基于ImagesPipeLine的一个管道类
                    get_media_request()
                    file_path
                    item_complete
                在配置文件中（settings）：
                    指定图片的存储目录
                    指定开启的管道
    中间件的使用(middlewares是中间件的文件)
        在middlewares中的两个类分别是爬虫中间件和下载中间件
        引擎和下载器（下载中间件），引擎和Spider之间都有一个中间件（爬虫中间件）
        下载中间件：
            批量拦截整个工程中所有的请求和响应
                拦截请求：
                    1。UA伪装（在settings文件中的配置和在中间件的效果是不同的）
                        中间件可以给拦截的请求设置为不同的样式，而settings文件中的是固定的相同的
                    2。代理ip的设定
                        给每一个请求设定不同的代理ip
                拦截响应：
                    1。篡改响应数据或者是响应对象（和动态响应有关）
                    2。在用selenium进行动态数据爬取时需要控制滚轮把动态数据全部加载出来


    CrawlSpider（类，是spider的子类）
        全站数据爬取的方式：
            基于spider实现，手动请求发送
            基于crawspider全站数据爬取
        CrawSpider的使用
            创建一个工程
            cd xxx
            -------------------------------------------------------------------------
            |创建爬虫文件（CrawSpider）：scrapy genspider -t crawl 爬虫文件名 www.xxx.com |
            --------------------------------------------------------------------------
            链接提取器：根据指定的规则（allow）进行指定链接的提取
            规则解析器：将链接提取器提取到的链接进行指定规则（callback）的解析


    分布式爬虫（了解）
        概念：需要搭建分布式机群（多台电脑搭建）让其对一组资源进行分布联合爬取
        作用：提升爬取数据效率
        如何实现分布式：
            安装scrapy-redis的组件
            原生的scrapy是实现不了分布式爬虫，必须让scrapy和scrapy-redis组件一起实现分布式爬虫


    增量式爬虫
        概念：监测网站数据更新情况，只会爬取网站最新更新出来的数据
        分析：
            指定一个起始的url
            基于crawlscrapy获取其他网页页码链接
            基于Rule将其他页码链接进行请求
            从每一个页码对应的页面源码中解析出每一个电影详情页的url
            核心：检测电影详情页的url之前有没有请求过
                对爬过详情页的url进行存储
                存储到redis的set数据结构
            对详情页的url发起请求，然后解析出电影的名称和简介
            进行持久化存储
















'''
